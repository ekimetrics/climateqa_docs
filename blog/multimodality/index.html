<!doctype html>
<html lang="en" dir="ltr" class="plugin-blog plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.1.1">
<title data-rh="true">ClimateQ&amp;A now features image interpretation | ClimateQ&amp;A</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://climateqa.com/img/screenshots/multimodal1.png"><meta data-rh="true" name="twitter:image" content="https://climateqa.com/img/screenshots/multimodal1.png"><meta data-rh="true" property="og:url" content="https://climateqa.com/blog/multimodality"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="ClimateQ&amp;A now features image interpretation | ClimateQ&amp;A"><meta data-rh="true" name="description" content="ClimateQ&amp;A version 1.3 introduces a new feature that enhances explanations with relevant images, significantly improving the interpretation of complex scientific reports from sources like the IPCC and IPBES. This feature, still in its experimental phase, leverages multimodal Large Language Models (LLMs) to index image descriptions into a search engine."><meta data-rh="true" property="og:description" content="ClimateQ&amp;A version 1.3 introduces a new feature that enhances explanations with relevant images, significantly improving the interpretation of complex scientific reports from sources like the IPCC and IPBES. This feature, still in its experimental phase, leverages multimodal Large Language Models (LLMs) to index image descriptions into a search engine."><meta data-rh="true" name="keywords" content="ClimateQ&amp;A,Climate Change"><link data-rh="true" rel="icon" href="/img/logo_climateqa.png"><link data-rh="true" rel="canonical" href="https://climateqa.com/blog/multimodality"><link data-rh="true" rel="alternate" href="https://climateqa.com/blog/multimodality" hreflang="en"><link data-rh="true" rel="alternate" href="https://climateqa.com/blog/multimodality" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="ClimateQ&amp;A RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="ClimateQ&amp;A Atom Feed"><link rel="stylesheet" href="/assets/css/styles.454c9aff.css">
<script src="/assets/js/runtime~main.1ff9761c.js" defer="defer"></script>
<script src="/assets/js/main.93f00fb8.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo_climateqa.png" alt="ClimateQ&amp;A Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/logo_climateqa.png" alt="ClimateQ&amp;A Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">ClimateQ&amp;A</b></a><a class="navbar__item navbar__link" href="/docs/intro/">About ClimateQ&amp;A</a><a class="navbar__item navbar__link" href="/docs/sources">Sources</a><a class="navbar__item navbar__link" href="/docs/contact">Contact</a><a class="navbar__item navbar__link" href="/docs/category/changelog">Changelog</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://huggingface.co/spaces/Ekimetrics/climate-question-answering/tree/main" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 blog-wrapper"><div class="container container-wide margin-vert--lg"><div class="row"><div class="col col--2"></div><main class="col col--8"><article><header><h1 class="margin-bottom--sm blogPostTitle_uMeh">ClimateQ&amp;A now features image interpretation</h1><div class="margin-vert--md"><p>ClimateQ&amp;A version 1.3 introduces a new feature that enhances explanations with relevant images, significantly improving the interpretation of complex scientific reports from sources like the IPCC and IPBES. This feature, still in its experimental phase, leverages multimodal Large Language Models (LLMs) to index image descriptions into a search engine.</p><time datetime="2024-02-20T00:00:00.000Z" class="blogPostDate_Ocfp">February<!-- --> <!-- -->20<!-- -->, <!-- -->2024<!-- --> <!-- --> · <!-- -->4<!-- --> min read</time></div><div class="avatar margin-vert--md"><div class="avatar__intro"><h4 class="avatar__name">Written by <a href="theo.alvesdacosta@ekimetrics.com" target="_blank" rel="noreferrer noopener">Théo ALVES DA COSTA</a></h4><small class="avatar__subtitle"></small></div></div><div class="margin-vert--md"><img class="img-blog-header" src="/img/screenshots/multimodal1.png"></div></header><section class="markdown blog-article-custom"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="abstract">Abstract<a href="#abstract" class="hash-link" aria-label="Direct link to Abstract" title="Direct link to Abstract">​</a></h2>
<p>The article discusses the innovative feature introduced in ClimateQ&amp;A version 1.3, which enables the platform to display images alongside explanations, enhancing the user&#x27;s understanding of complex scientific data. This feature is particularly useful for interpreting scientific reports, such as those from the IPCC and IPBES, which are rich in illustrative images designed to communicate scientific outcomes.</p>
<p>To implement this capability, the system indexes descriptions generated by multimodal Large Language Models (LLMs) into its search engine. For each image, the text before and after it is also captured to provide context, often necessary as the following text frequently acts as the image&#x27;s caption. These descriptions are then generated using the GPT4Vision API and converted into vectors using the same embedding model as the text, making them searchable within the platform.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="climateqa-now-features-image-interpretation">ClimateQ&amp;A now features image interpretation<a href="#climateqa-now-features-image-interpretation" class="hash-link" aria-label="Direct link to ClimateQ&amp;A now features image interpretation" title="Direct link to ClimateQ&amp;A now features image interpretation">​</a></h2>
<div class="theme-admonition theme-admonition-warning admonition_xJq3 alert alert--warning"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>Experimental</div><div class="admonitionContent_BuS1"><p>This feature is still in the experimental phase and may undergo changes in the coming months.</p></div></div>
<p>Starting from <a href="/docs/changelog/v1.3.0">version 1.3</a>, ClimateQ&amp;A now offers the capability to include images alongside explanations.</p>
<p>The image is displayed within the chat interface at the end of the message, as illustrated above.</p>
<p>This &quot;image interpretation&quot; feature was highly requested, given that scientific reports, such as those from the IPCC and IPBES, often incorporate numerous images designed to convey complex scientific results.</p>
<p>Of course, not all inquiries will return images; the system only provides images when they are relevant to the explanation. Likely images from the referenced passages are accessible under the &quot;Figures&quot; tab.</p>
<p><img decoding="async" loading="lazy" src="/assets/images/fig2-dce111396465ec6577d364af5e6a27e3.png" width="1506" height="567" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-it-works">How it works<a href="#how-it-works" class="hash-link" aria-label="Direct link to How it works" title="Direct link to How it works">​</a></h2>
<p>To implement this feature, we index descriptions generated by multimodal Large Language Models (LLMs) in our search engine.</p>
<ul>
<li>For this initial release, during the phase of parsing scientific reports, we independently extracted each image (recalling that the document segmentation and parsing are performed using a YOLO model fine-tuned on the <a href="https://github.com/DS4SD/DocLayNet" target="_blank" rel="noopener noreferrer">DocLayNet dataset</a>).</li>
<li>For each image, we also retrieve the preceding and following text. This is done to provide the multimodal LLM with the context in which the image is situated, assuming that the following text often serves as the image&#x27;s caption.</li>
<li>We then use the GPT4-Vision API to generate descriptions from the image and context texts, using the following prompt:</li>
</ul>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">You are going to act as a caption generator. Your goal is NOT to generate a description of the image but more the summary and explanation it conveys. This description will be indexed in a search engine to be found based on a user query.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">The images are drawn from the scientific reports of the IPCC on climate change and IPBES on biodiversity. So they are mostly complex science schemas.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">If the image is weird, is a logo, looks like it has been wrongly cropped, or you don&#x27;t know what it is, just return &quot;unknown&quot;. </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">I also provide the context for the image by giving you the text coming before the image in the document, and the text coming after the image (often a caption for the figure, so it may be useful).</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Text before:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">{text_before}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Text after:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">{text_after}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Summary:</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<ul>
<li>
<p>The description is then converted into vectors using the same embedding model as the text and added to the vector store to be searchable in the same way as text chunks.</p>
</li>
<li>
<p>Then, at the time of the final synthesis in the interface in response to a question, if one of the k retrieved chunks is an image, we display it at the end, while indicating to the user that the description was AI-generated.
<img decoding="async" loading="lazy" src="/assets/images/fig1-50a833f83c568fc90449f2fd0aa8c1e7.png" width="1491" height="829" class="img_ev3q"></p>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="next-steps">Next Steps<a href="#next-steps" class="hash-link" aria-label="Direct link to Next Steps" title="Direct link to Next Steps">​</a></h2>
<p>This is an initial, still experimental feature, but the results are already satisfactory enough to continue work in this direction. Our next steps to improve this feature include:</p>
<ul>
<li>Expanding the number of images covered; currently, not all images are analyzed.</li>
<li>Moving to an open-source model like LLaVA to experiment more quickly and allow for easier replication of experiments.</li>
<li>Directly indexing figures indexed by the IPCC (e.g., <a href="https://www.ipcc.ch/report/ar6/syr/figures/" target="_blank" rel="noopener noreferrer">https://www.ipcc.ch/report/ar6/syr/figures/</a>).</li>
<li>Better integrate images in the response and return several images.</li>
<li>Improving the caption identification of our detection model.</li>
</ul></section><footer class="row margin-vert--lg"><div class="col"><strong>Tags:</strong><a class="margin-horiz--sm" href="/blog/tags/climate-q-a">ClimateQ&amp;A</a><a class="margin-horiz--sm" href="/blog/tags/multi-modality">Multi-modality</a></div></footer></article></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#abstract" class="table-of-contents__link toc-highlight">Abstract</a></li><li><a href="#climateqa-now-features-image-interpretation" class="table-of-contents__link toc-highlight">ClimateQ&amp;A now features image interpretation</a></li><li><a href="#how-it-works" class="table-of-contents__link toc-highlight">How it works</a></li><li><a href="#next-steps" class="table-of-contents__link toc-highlight">Next Steps</a></li></ul></div></div></div></div></div><footer class="footer"><div class="container container-fluid"></div></footer></div>
</body>
</html>